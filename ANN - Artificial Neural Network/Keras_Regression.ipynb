{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>461.527929</td>\n",
       "      <td>999.787558</td>\n",
       "      <td>999.766096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>548.130011</td>\n",
       "      <td>998.861615</td>\n",
       "      <td>1001.042403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>410.297162</td>\n",
       "      <td>1000.070267</td>\n",
       "      <td>998.844015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>540.382220</td>\n",
       "      <td>999.952251</td>\n",
       "      <td>1000.440940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>546.024553</td>\n",
       "      <td>1000.446011</td>\n",
       "      <td>1000.338531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        price     feature1     feature2\n",
       "0  461.527929   999.787558   999.766096\n",
       "1  548.130011   998.861615  1001.042403\n",
       "2  410.297162  1000.070267   998.844015\n",
       "3  540.382220   999.952251  1000.440940\n",
       "4  546.024553  1000.446011  1000.338531"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fake_reg.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow works only on numpy arrays.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['feature1','feature2']].values\n",
    "y = df[['price']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MinMaxScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class MinMaxScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  MinMaxScaler(feature_range=(0, 1), *, copy=True)\n",
      " |  \n",
      " |  Transform features by scaling each feature to a given range.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that it is in the given range on the training set, e.g. between\n",
      " |  zero and one.\n",
      " |  \n",
      " |  The transformation is given by::\n",
      " |  \n",
      " |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      " |      X_scaled = X_std * (max - min) + min\n",
      " |  \n",
      " |  where min, max = feature_range.\n",
      " |  \n",
      " |  This transformation is often used as an alternative to zero mean,\n",
      " |  unit variance scaling.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  feature_range : tuple (min, max), default=(0, 1)\n",
      " |      Desired range of transformed data.\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      Set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  min_ : ndarray of shape (n_features,)\n",
      " |      Per feature adjustment for minimum. Equivalent to\n",
      " |      ``min - X.min(axis=0) * self.scale_``\n",
      " |  \n",
      " |  scale_ : ndarray of shape (n_features,)\n",
      " |      Per feature relative scaling of the data. Equivalent to\n",
      " |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  data_min_ : ndarray of shape (n_features,)\n",
      " |      Per feature minimum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_min_*\n",
      " |  \n",
      " |  data_max_ : ndarray of shape (n_features,)\n",
      " |      Per feature maximum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_max_*\n",
      " |  \n",
      " |  data_range_ : ndarray of shape (n_features,)\n",
      " |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_range_*\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator.\n",
      " |      It will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      " |  >>> scaler = MinMaxScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  MinMaxScaler()\n",
      " |  >>> print(scaler.data_max_)\n",
      " |  [ 1. 18.]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[0.   0.  ]\n",
      " |   [0.25 0.25]\n",
      " |   [0.5  0.5 ]\n",
      " |   [1.   1.  ]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[1.5 0. ]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  minmax_scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MinMaxScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_range=(0, 1), *, copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the minimum and maximum to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Undo the scaling of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed. It cannot be sparse.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of min and max on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Transformer instance.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scale features of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "help(MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54249473,  0.54328931],\n",
       "       [ 0.40972853,  0.67589824],\n",
       "       [ 0.89792435,  0.38027611],\n",
       "       [ 0.56655528,  0.54066167],\n",
       "       [ 0.83569889,  0.48343275],\n",
       "       [ 0.5473575 ,  0.61205685],\n",
       "       [ 0.40458321,  0.70748271],\n",
       "       [ 0.70809883,  0.47921342],\n",
       "       [ 0.54534139,  0.57919334],\n",
       "       [ 0.74233558,  0.63521355],\n",
       "       [ 0.45278522,  0.56198635],\n",
       "       [ 0.31408355,  0.86107829],\n",
       "       [ 0.31633559,  0.45932243],\n",
       "       [ 0.38448211,  0.28880237],\n",
       "       [ 0.45227209,  0.3745113 ],\n",
       "       [ 0.87778381,  0.51227052],\n",
       "       [ 0.32697439,  0.3101695 ],\n",
       "       [ 0.83032833,  0.57119645],\n",
       "       [ 0.4003136 ,  0.53631146],\n",
       "       [ 0.69886924,  0.45760106],\n",
       "       [ 0.54002644,  0.16705268],\n",
       "       [ 0.66135985,  0.65416255],\n",
       "       [ 0.30079292,  0.70903396],\n",
       "       [ 0.41162087,  0.49976822],\n",
       "       [ 0.39900912,  0.90274866],\n",
       "       [ 1.05249438,  0.60215171],\n",
       "       [ 0.34307704,  0.89164054],\n",
       "       [ 0.40883101,  0.49638204],\n",
       "       [ 0.50470629,  0.38250272],\n",
       "       [ 0.31876436,  0.71160441],\n",
       "       [ 0.66899466,  0.74217104],\n",
       "       [ 0.57123557,  0.64552838],\n",
       "       [ 0.73320911,  0.48786698],\n",
       "       [ 0.325459  ,  0.33395525],\n",
       "       [ 0.54643666,  0.80675016],\n",
       "       [ 0.61690173,  0.39545579],\n",
       "       [ 0.7154035 ,  0.61043085],\n",
       "       [ 0.37179794,  0.24832903],\n",
       "       [ 0.18613366,  0.40197077],\n",
       "       [ 0.359893  ,  0.34164062],\n",
       "       [ 0.72544709,  0.61343427],\n",
       "       [ 0.31766198,  0.53003646],\n",
       "       [ 0.14282758,  0.56148208],\n",
       "       [ 0.50607934,  0.37199119],\n",
       "       [ 0.54161699,  0.68454423],\n",
       "       [ 0.63891744,  0.01847569],\n",
       "       [ 0.6092789 ,  0.25137496],\n",
       "       [ 0.44576252,  0.60308584],\n",
       "       [ 0.75818659,  0.42437466],\n",
       "       [ 0.41064011,  0.61156373],\n",
       "       [ 0.45019364,  0.76416114],\n",
       "       [ 0.81394911,  0.49905609],\n",
       "       [ 0.50345372, -0.00707422],\n",
       "       [ 0.4467854 ,  0.51769178],\n",
       "       [ 0.85624035,  0.45954591],\n",
       "       [ 0.28806386,  0.35415584],\n",
       "       [ 0.52640367,  0.38365927],\n",
       "       [ 0.64179995,  0.58191749],\n",
       "       [ 0.42666694,  0.62372211],\n",
       "       [ 0.70079542,  0.57646527],\n",
       "       [ 0.55865236,  0.23946955],\n",
       "       [ 0.65835711,  0.72944419],\n",
       "       [ 0.54041127,  0.53842956],\n",
       "       [ 0.60935139,  0.32689387],\n",
       "       [ 0.50333898,  0.50667526],\n",
       "       [ 0.76410778,  0.40937998],\n",
       "       [ 0.53645939,  0.61885066],\n",
       "       [ 0.397778  ,  0.59895178],\n",
       "       [ 0.35294975,  0.52967544],\n",
       "       [ 0.47316483,  0.47529213],\n",
       "       [ 1.08706489,  0.68154445],\n",
       "       [ 0.51750296,  0.53220938],\n",
       "       [ 0.67216002,  0.64279746],\n",
       "       [ 0.52116141,  0.59110087],\n",
       "       [ 0.68295704,  0.50009905],\n",
       "       [ 0.6742071 ,  0.06259156],\n",
       "       [ 0.45650122,  0.3880659 ],\n",
       "       [ 0.29321402,  0.62879665],\n",
       "       [ 0.63255598,  0.45675314],\n",
       "       [ 0.34312164,  0.44884223],\n",
       "       [ 0.78610855,  0.49035656],\n",
       "       [ 0.47530593,  0.18072462],\n",
       "       [ 0.60835493,  0.38580479],\n",
       "       [ 0.21572832,  0.37999193],\n",
       "       [ 0.45013726,  0.33208455],\n",
       "       [ 0.56422655,  0.76857022],\n",
       "       [ 0.39954514,  0.42181592],\n",
       "       [ 0.37241424,  0.23600581],\n",
       "       [ 0.56411042,  0.54958881],\n",
       "       [ 0.56512983,  0.67406614],\n",
       "       [ 0.64035879,  0.4770078 ],\n",
       "       [ 0.65368986,  0.55057928],\n",
       "       [ 0.89650264,  0.67148914],\n",
       "       [ 0.3647678 ,  0.49640988],\n",
       "       [ 0.42648735,  0.65109322],\n",
       "       [ 0.58916962,  0.23649055],\n",
       "       [ 0.70016434,  0.32550019],\n",
       "       [ 0.29625452,  0.41613865],\n",
       "       [ 0.61861061,  0.5361263 ],\n",
       "       [ 0.89037817,  0.48443553],\n",
       "       [ 0.36654359,  0.43223859],\n",
       "       [ 0.69492993,  0.66356446],\n",
       "       [ 0.46131129,  0.6560011 ],\n",
       "       [ 0.47567129,  0.85895439],\n",
       "       [ 0.83554224,  0.83468009],\n",
       "       [ 0.42769474,  0.40673378],\n",
       "       [ 0.4596815 ,  0.28938313],\n",
       "       [ 0.38856872,  0.67017632],\n",
       "       [ 0.63405975,  0.42295346],\n",
       "       [ 0.66500551,  0.67720254],\n",
       "       [ 0.67963749,  0.8977563 ],\n",
       "       [ 0.61780011,  0.22862634],\n",
       "       [ 0.24498314,  0.4907566 ],\n",
       "       [ 0.56517298,  0.50963638],\n",
       "       [ 0.76933853,  0.23430741],\n",
       "       [ 0.42637115,  0.85864335],\n",
       "       [ 0.36032715,  0.91021646],\n",
       "       [ 0.68156128,  0.68848503],\n",
       "       [ 0.46062899,  0.37665815],\n",
       "       [ 0.60621323,  0.40594541],\n",
       "       [ 0.33683423,  0.65545238],\n",
       "       [ 0.71320422,  0.69662808],\n",
       "       [ 0.44424637,  0.75698267],\n",
       "       [ 0.52683923,  0.7211757 ],\n",
       "       [ 0.56860768,  0.19640573],\n",
       "       [ 0.34494725,  0.79016669],\n",
       "       [ 0.30831425,  0.29145534],\n",
       "       [ 0.45678636,  0.35966401],\n",
       "       [ 0.51027281,  0.42376057],\n",
       "       [ 0.33036406,  0.63560494],\n",
       "       [ 0.79198701,  0.55691024],\n",
       "       [ 0.5682181 ,  0.38136964],\n",
       "       [ 0.65395701,  0.50620131],\n",
       "       [ 0.22745828,  0.43998253],\n",
       "       [ 0.70178824,  0.31774162],\n",
       "       [ 0.257568  ,  0.68475293],\n",
       "       [ 0.6687606 ,  0.66904452],\n",
       "       [ 0.52293269,  0.2558262 ],\n",
       "       [ 0.82808532,  0.58871022],\n",
       "       [ 0.31547675,  0.67690515],\n",
       "       [ 0.50426439,  0.59918921],\n",
       "       [ 0.77438864,  0.56057212],\n",
       "       [ 0.63189942,  0.69505049],\n",
       "       [ 0.28020049,  0.84306732],\n",
       "       [ 0.90903   ,  0.83620157],\n",
       "       [ 0.65176758,  0.21426002],\n",
       "       [ 0.63870258,  0.3876855 ],\n",
       "       [ 0.7689242 ,  0.37133859],\n",
       "       [ 0.58086589,  0.4484026 ],\n",
       "       [ 0.64872388,  0.72369121],\n",
       "       [ 0.4951191 ,  0.55548677],\n",
       "       [ 0.45842964,  0.53713744],\n",
       "       [ 0.45778621,  0.30501287],\n",
       "       [ 0.8273114 ,  0.20651043],\n",
       "       [ 0.60969813,  0.37789192],\n",
       "       [ 0.82488016,  0.63991594],\n",
       "       [ 0.43893179,  0.38335656],\n",
       "       [ 0.49197669,  0.42877216],\n",
       "       [ 0.60376251,  0.78317921],\n",
       "       [ 0.4848904 ,  0.35291469],\n",
       "       [ 0.59662074,  0.21165816],\n",
       "       [ 0.48747217,  0.3971717 ],\n",
       "       [ 0.51675829,  0.54958519],\n",
       "       [ 0.59893971,  0.45620644],\n",
       "       [ 0.70710141,  0.5415314 ],\n",
       "       [ 0.26410196,  0.34190131],\n",
       "       [ 0.39557668,  0.68384183],\n",
       "       [ 0.73045215,  0.57020925],\n",
       "       [ 0.48699059,  0.59311226],\n",
       "       [ 0.44326881,  0.52936004],\n",
       "       [ 0.53472395,  0.40836655],\n",
       "       [ 0.55026592,  0.50405858],\n",
       "       [ 0.49420465,  0.54377183],\n",
       "       [ 0.40160266,  0.61820684],\n",
       "       [ 0.65696162,  0.6580019 ],\n",
       "       [ 0.47456593,  0.18359486],\n",
       "       [ 0.15362748,  0.33309355],\n",
       "       [ 0.45420029,  0.61967192],\n",
       "       [ 0.59447989,  0.72218422],\n",
       "       [ 0.58965575,  0.47447909],\n",
       "       [ 0.5954944 ,  0.50617829],\n",
       "       [ 0.38428055,  0.78424104],\n",
       "       [ 0.76401822,  0.36351929],\n",
       "       [ 0.5449867 ,  0.77174723],\n",
       "       [ 0.714259  ,  0.2709664 ],\n",
       "       [ 0.31127924,  0.48940247],\n",
       "       [ 0.54264787,  0.60300867],\n",
       "       [ 0.60350823,  0.29735622],\n",
       "       [ 0.38785522,  0.58474425],\n",
       "       [ 0.63094411,  0.48311668],\n",
       "       [ 0.76855537,  0.42483968],\n",
       "       [ 0.45900703,  0.53298953],\n",
       "       [ 0.31893272,  0.37670096],\n",
       "       [ 0.46576111,  0.63801003],\n",
       "       [ 0.46439331,  0.75872443],\n",
       "       [ 0.20684472,  0.60106431],\n",
       "       [ 0.20756829,  0.70810556],\n",
       "       [ 0.68291768,  0.7151698 ],\n",
       "       [ 0.32070801,  0.42203289],\n",
       "       [ 0.01512337,  0.49232367]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_train)\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Ways of creating a keras \n",
    "# 1st Method.\n",
    "# Here we are creating layers of  1st layer - 4 neurons , 2nd layer - 2 neurons and finally an\n",
    "# Output layer of 1 neuron. and giving each hidden layer an activation function of rectified linear unit\n",
    "model = Sequential([Dense(4,activation='relu'),\n",
    "                    Dense(2,activation='relu'),\n",
    "                   Dense(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd Method\n",
    "model = Sequential()\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(2,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#Adding layers one by one \n",
    "#preferred method as we can easily add or remove layer thorugh this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4,activation = 'relu'))\n",
    "model.add(Dense(4,activation = 'relu'))\n",
    "model.add(Dense(4,activation = 'relu'))\n",
    "#Output Layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "#Compiling our neural network\n",
    "#optimizer - gradient descent optimizer\n",
    "#loss - loss function , mse - mean squared error\n",
    "model.compile(optimizer = 'rmsprop' , loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "25/25 [==============================] - 1s 2ms/step - loss: 257533.7031\n",
      "Epoch 2/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 257506.5781\n",
      "Epoch 3/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 257481.5000\n",
      "Epoch 4/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257456.5938\n",
      "Epoch 5/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257431.6406\n",
      "Epoch 6/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257406.7969\n",
      "Epoch 7/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257381.9062\n",
      "Epoch 8/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257356.8750\n",
      "Epoch 9/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257331.9531\n",
      "Epoch 10/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257307.0469\n",
      "Epoch 11/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257282.1406\n",
      "Epoch 12/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257257.1875\n",
      "Epoch 13/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257232.2812\n",
      "Epoch 14/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257207.3594\n",
      "Epoch 15/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257182.5156\n",
      "Epoch 16/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257157.5625\n",
      "Epoch 17/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 257132.6562\n",
      "Epoch 18/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 257107.7031\n",
      "Epoch 19/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257082.8594\n",
      "Epoch 20/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257057.9219\n",
      "Epoch 21/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257032.9531\n",
      "Epoch 22/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 257008.1250\n",
      "Epoch 23/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256983.2344\n",
      "Epoch 24/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256958.3125\n",
      "Epoch 25/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256933.3750\n",
      "Epoch 26/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256908.4844\n",
      "Epoch 27/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256883.5938\n",
      "Epoch 28/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256858.7188\n",
      "Epoch 29/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256833.8125\n",
      "Epoch 30/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256808.9219\n",
      "Epoch 31/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256784.0469\n",
      "Epoch 32/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256759.1875\n",
      "Epoch 33/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256734.2031\n",
      "Epoch 34/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256709.3438\n",
      "Epoch 35/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256684.5781\n",
      "Epoch 36/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256659.6406\n",
      "Epoch 37/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256634.7656\n",
      "Epoch 38/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256609.8438\n",
      "Epoch 39/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256584.8750\n",
      "Epoch 40/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256559.9375\n",
      "Epoch 41/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256535.1562\n",
      "Epoch 42/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256510.2969\n",
      "Epoch 43/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256485.4219\n",
      "Epoch 44/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256460.5156\n",
      "Epoch 45/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256435.6406\n",
      "Epoch 46/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256410.7812\n",
      "Epoch 47/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256385.9219\n",
      "Epoch 48/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256361.0625\n",
      "Epoch 49/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256336.2031\n",
      "Epoch 50/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256311.3125\n",
      "Epoch 51/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256286.4062\n",
      "Epoch 52/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256261.5469\n",
      "Epoch 53/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256236.6875\n",
      "Epoch 54/250\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 256211.7656\n",
      "Epoch 55/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256186.9844\n",
      "Epoch 56/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256162.2031\n",
      "Epoch 57/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256137.2812\n",
      "Epoch 58/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256112.4375\n",
      "Epoch 59/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256087.5156\n",
      "Epoch 60/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256062.7656\n",
      "Epoch 61/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256037.8125\n",
      "Epoch 62/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 256012.9219\n",
      "Epoch 63/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255988.0156\n",
      "Epoch 64/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255963.2969\n",
      "Epoch 65/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255938.5000\n",
      "Epoch 66/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 255913.5938\n",
      "Epoch 67/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255888.6875\n",
      "Epoch 68/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255863.8594\n",
      "Epoch 69/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255838.9844\n",
      "Epoch 70/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255814.0781\n",
      "Epoch 71/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255789.3125\n",
      "Epoch 72/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255764.4375\n",
      "Epoch 73/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255739.5625\n",
      "Epoch 74/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255714.7969\n",
      "Epoch 75/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255690.0156\n",
      "Epoch 76/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255665.1406\n",
      "Epoch 77/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255640.2656\n",
      "Epoch 78/250\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 255615.4375\n",
      "Epoch 79/250\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 255590.6406\n",
      "Epoch 80/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 255565.7969\n",
      "Epoch 81/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 255540.9219\n",
      "Epoch 82/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 255516.1562\n",
      "Epoch 83/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255491.2344\n",
      "Epoch 84/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255466.4844\n",
      "Epoch 85/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255441.5938\n",
      "Epoch 86/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255416.7969\n",
      "Epoch 87/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255391.9844\n",
      "Epoch 88/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255367.1406\n",
      "Epoch 89/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255342.2812\n",
      "Epoch 90/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255317.5156\n",
      "Epoch 91/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255292.6875\n",
      "Epoch 92/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255267.8594\n",
      "Epoch 93/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255243.0781\n",
      "Epoch 94/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255218.2188\n",
      "Epoch 95/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255193.4062\n",
      "Epoch 96/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 2ms/step - loss: 255168.6875\n",
      "Epoch 97/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255143.8438\n",
      "Epoch 98/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255119.0625\n",
      "Epoch 99/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255094.2344\n",
      "Epoch 100/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255069.3750\n",
      "Epoch 101/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255044.5156\n",
      "Epoch 102/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 255019.7031\n",
      "Epoch 103/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254994.9375\n",
      "Epoch 104/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254970.1406\n",
      "Epoch 105/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254945.4531\n",
      "Epoch 106/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254920.5938\n",
      "Epoch 107/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254895.7812\n",
      "Epoch 108/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254871.0156\n",
      "Epoch 109/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254846.2031\n",
      "Epoch 110/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254821.4219\n",
      "Epoch 111/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254796.6250\n",
      "Epoch 112/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254771.7969\n",
      "Epoch 113/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254747.0000\n",
      "Epoch 114/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254722.2188\n",
      "Epoch 115/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254697.4219\n",
      "Epoch 116/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254672.7031\n",
      "Epoch 117/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254647.9062\n",
      "Epoch 118/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254623.0781\n",
      "Epoch 119/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254598.2969\n",
      "Epoch 120/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254573.5625\n",
      "Epoch 121/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254548.8438\n",
      "Epoch 122/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254524.0469\n",
      "Epoch 123/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254499.1562\n",
      "Epoch 124/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254474.3438\n",
      "Epoch 125/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254449.5938\n",
      "Epoch 126/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254424.8438\n",
      "Epoch 127/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254400.0781\n",
      "Epoch 128/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254375.3594\n",
      "Epoch 129/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254350.5781\n",
      "Epoch 130/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254325.8438\n",
      "Epoch 131/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254300.9531\n",
      "Epoch 132/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254276.2344\n",
      "Epoch 133/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254251.4531\n",
      "Epoch 134/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254226.7344\n",
      "Epoch 135/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254201.9219\n",
      "Epoch 136/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254177.1562\n",
      "Epoch 137/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254152.4375\n",
      "Epoch 138/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254127.7188\n",
      "Epoch 139/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254102.9375\n",
      "Epoch 140/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254078.1406\n",
      "Epoch 141/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254053.3594\n",
      "Epoch 142/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254028.6406\n",
      "Epoch 143/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 254003.9062\n",
      "Epoch 144/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253979.1562\n",
      "Epoch 145/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253954.4062\n",
      "Epoch 146/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253929.6406\n",
      "Epoch 147/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253904.9375\n",
      "Epoch 148/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253880.1562\n",
      "Epoch 149/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253855.4062\n",
      "Epoch 150/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253830.6406\n",
      "Epoch 151/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253805.9219\n",
      "Epoch 152/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253781.1562\n",
      "Epoch 153/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253756.4844\n",
      "Epoch 154/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253731.7344\n",
      "Epoch 155/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253706.9844\n",
      "Epoch 156/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253682.2344\n",
      "Epoch 157/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253657.5000\n",
      "Epoch 158/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253632.7969\n",
      "Epoch 159/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253608.0781\n",
      "Epoch 160/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253583.2969\n",
      "Epoch 161/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253558.5156\n",
      "Epoch 162/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253533.8125\n",
      "Epoch 163/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253509.0938\n",
      "Epoch 164/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 253484.3750\n",
      "Epoch 165/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253459.6406\n",
      "Epoch 166/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253434.9219\n",
      "Epoch 167/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253410.2031\n",
      "Epoch 168/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253385.4375\n",
      "Epoch 169/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253360.7188\n",
      "Epoch 170/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253336.0156\n",
      "Epoch 171/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253311.3594\n",
      "Epoch 172/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253286.5938\n",
      "Epoch 173/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253261.8125\n",
      "Epoch 174/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253237.0781\n",
      "Epoch 175/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253212.4531\n",
      "Epoch 176/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253187.7344\n",
      "Epoch 177/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253163.0469\n",
      "Epoch 178/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253138.3438\n",
      "Epoch 179/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253113.6875\n",
      "Epoch 180/250\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 253088.9531\n",
      "Epoch 181/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253064.2188\n",
      "Epoch 182/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 253039.5469\n",
      "Epoch 183/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 253014.8438\n",
      "Epoch 184/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252990.0781\n",
      "Epoch 185/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252965.2812\n",
      "Epoch 186/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 252940.7031\n",
      "Epoch 187/250\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 252916.0000\n",
      "Epoch 188/250\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 252891.2656\n",
      "Epoch 189/250\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 252866.5781\n",
      "Epoch 190/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step - loss: 252841.9219\n",
      "Epoch 191/250\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 252817.2031\n",
      "Epoch 192/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252792.5156\n",
      "Epoch 193/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 252767.8594\n",
      "Epoch 194/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252743.1250\n",
      "Epoch 195/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252718.4062\n",
      "Epoch 196/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252693.6562\n",
      "Epoch 197/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252668.9844\n",
      "Epoch 198/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252644.2969\n",
      "Epoch 199/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252619.7188\n",
      "Epoch 200/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252595.0469\n",
      "Epoch 201/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252570.3438\n",
      "Epoch 202/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252545.6406\n",
      "Epoch 203/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252521.0156\n",
      "Epoch 204/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252496.2969\n",
      "Epoch 205/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252471.5938\n",
      "Epoch 206/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252446.9375\n",
      "Epoch 207/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252422.3594\n",
      "Epoch 208/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252397.5625\n",
      "Epoch 209/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252372.9375\n",
      "Epoch 210/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252348.2969\n",
      "Epoch 211/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252323.6406\n",
      "Epoch 212/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252298.9375\n",
      "Epoch 213/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252274.2031\n",
      "Epoch 214/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252249.4844\n",
      "Epoch 215/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252224.7969\n",
      "Epoch 216/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252200.1250\n",
      "Epoch 217/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252175.5156\n",
      "Epoch 218/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252150.9375\n",
      "Epoch 219/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252126.2656\n",
      "Epoch 220/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252101.6562\n",
      "Epoch 221/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252077.0000\n",
      "Epoch 222/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252052.3750\n",
      "Epoch 223/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252027.6406\n",
      "Epoch 224/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 252003.0156\n",
      "Epoch 225/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251978.3438\n",
      "Epoch 226/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251953.7031\n",
      "Epoch 227/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251929.0781\n",
      "Epoch 228/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251904.4375\n",
      "Epoch 229/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251879.7812\n",
      "Epoch 230/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251855.0938\n",
      "Epoch 231/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251830.4375\n",
      "Epoch 232/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251805.8438\n",
      "Epoch 233/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251781.2344\n",
      "Epoch 234/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251756.5781\n",
      "Epoch 235/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251731.9062\n",
      "Epoch 236/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251707.2812\n",
      "Epoch 237/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251682.7031\n",
      "Epoch 238/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251657.9531\n",
      "Epoch 239/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251633.4219\n",
      "Epoch 240/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251608.7188\n",
      "Epoch 241/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251584.1406\n",
      "Epoch 242/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 251559.5469\n",
      "Epoch 243/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 251534.9219\n",
      "Epoch 244/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 251510.2656\n",
      "Epoch 245/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 251485.6406\n",
      "Epoch 246/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 251460.9375\n",
      "Epoch 247/250\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 251436.4062\n",
      "Epoch 248/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251411.7969\n",
      "Epoch 249/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251387.1250\n",
      "Epoch 250/250\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 251362.4844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1315e361e80>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model \n",
    "#How many times our model go through the dataset - epochs\n",
    "#verbose - information displayed during fitting\n",
    "model.fit(x=X_train,y=y_train,epochs=250)\n",
    "#On each epochs , loss gets minimized as our weights and biases in the network gets adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x131658f4040>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To know about the loss at each epoch\n",
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250911.515625"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491.45950827558175"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error , mean_squared_error\n",
    "mean_absolute_error(y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250911.50244762257"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.2420554]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting on new feature\n",
    "new_gem = [[998,1000]]\n",
    "new_gem = scaler.transform(new_gem)\n",
    "model.predict(new_gem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the trained model in keras\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('my_gem_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.2420554]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the saved model later in another notebook\n",
    "later_model = load_model('my_gem_model.h5')\n",
    "#And we can do all the prediction like task on the model loaded.\n",
    "later_model.predict(new_gem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
